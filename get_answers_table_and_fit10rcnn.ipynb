{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BorisLoveDev/agents-of-uniform-difficulty/blob/main/get_answers_table_and_fit10rcnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "-LESJJ9u7bjU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Подключение Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kNm_F8qC7dug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8913c9a0-2337-4a87-b9bb-7c53db4bf435"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка и нормализация данных MNIST\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# Добавление измерения канала к данным\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "# Применение one-hot encoding\n",
        "y_test_ohe = to_categorical(y_test, num_classes)\n",
        "y_train_ohe = to_categorical(y_train, num_classes)\n",
        "y_train_ohe.shape"
      ],
      "metadata": {
        "id": "p6SUnaDC7ogF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f32c20e5-040e-44d7-9fd7-155c557b6657"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных для уровня сложности 0\n",
        "x_data_level_0 = np.load('/content/drive/MyDrive/all_difficulty_data_mnist/x_data_difficult_level_0.npy')\n",
        "y_data_level_0 = np.load('/content/drive/MyDrive/all_difficulty_data_mnist/y_data_difficult_level_0.npy')\n",
        "\n",
        "# Загрузка данных для уровня сложности 6\n",
        "x_data_level_6 = np.load('/content/drive/MyDrive/all_difficulty_data_mnist/x_data_difficult_level_6.npy')\n",
        "y_data_level_6 = np.load('/content/drive/MyDrive/all_difficulty_data_mnist/y_data_difficult_level_6.npy')\n"
      ],
      "metadata": {
        "id": "9MoPMuvofpYT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для деформации изображения, как определено ранее\n",
        "def deform_image_optimized(image, A, B, M=28, NP=5):\n",
        "    C = M / (NP + 1.0)\n",
        "    XN, YN = np.zeros(M), np.zeros(M)\n",
        "    DX, DY = np.linspace(0, M-1, M), np.linspace(0, M-1, M)\n",
        "\n",
        "    for j in range(NP):\n",
        "        TXN = (j + 0.5 - np.random.random()) * C\n",
        "        TYN = (j + 0.5 - np.random.random()) * C\n",
        "        TDX = (j + 0.5 - np.random.random()) * C\n",
        "        TDY = (j + 0.5 - np.random.random()) * C\n",
        "        AXN = B * (1.0 - 2.0 * np.random.random())\n",
        "        AYN = B * (1.0 - 2.0 * np.random.random())\n",
        "        ADX = A * (1.0 - 2.0 * np.random.random())\n",
        "        ADY = A * (1.0 - 2.0 * np.random.random())\n",
        "        PXN = (0.1 + 0.9 * np.random.random()) * C\n",
        "        PYN = (0.1 + 0.9 * np.random.random()) * C\n",
        "        PDX = (0.1 + 0.9 * np.random.random()) * C\n",
        "        PDY = (0.1 + 0.9 * np.random.random()) * C\n",
        "\n",
        "        DX += ADX * np.exp(-((DX - TDX) / PDX)**2)\n",
        "        DY += ADY * np.exp(-((DY - TDY) / PDY)**2)\n",
        "        XN += AXN * np.exp(-((DX - TXN) / PXN)**2)\n",
        "        YN += AYN * np.exp(-((DY - TYN) / PYN)**2)\n",
        "\n",
        "    deformed_image = np.zeros((M, M))\n",
        "    for j in range(M):\n",
        "        for i in range(M):\n",
        "            x_index = int(DX[i] + XN[j])\n",
        "            y_index = int(DY[j] + YN[i])\n",
        "            if 0 <= x_index < M and 0 <= y_index < M:\n",
        "                deformed_image[j, i] = image[y_index, x_index] if y_index < image.shape[0] and x_index < image.shape[1] else 0\n",
        "\n",
        "    return deformed_image"
      ],
      "metadata": {
        "id": "IDDXpVxmATsD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deform_params = {\n",
        "    \"1\": (0.2, 1.0),\n",
        "    \"2\": (0.4, 3.0),\n",
        "    \"3\": (1, 5.0),\n",
        "    \"4\": (3.5, 9.5),\n",
        "    \"5\": (1, 5.0),\n",
        "}"
      ],
      "metadata": {
        "id": "faAey5shAUeY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получение таблицы ответов для 10 акторов"
      ],
      "metadata": {
        "id": "WbinBMGIMtGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, SeparableConv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, Dense, Dropout, Flatten, BatchNormalization, Activation, LeakyReLU, ELU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "kjYl2Tvlnno9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_random_cnn_model(input_shape=(28, 28, 1), num_classes=10):\n",
        "    model = Sequential()\n",
        "\n",
        "    conv_layer = random.choice([Conv2D, SeparableConv2D])\n",
        "    filters = random.randint(16, 64)\n",
        "    kernel_size = random.choice([(3, 3), (5, 5)])\n",
        "    activation = random.choice(['relu', 'leaky_relu', 'elu', 'selu', 'swish'])\n",
        "\n",
        "    model.add(conv_layer(filters=filters, kernel_size=kernel_size, activation=activation, input_shape=input_shape))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    num_layers = random.randint(1, 3)\n",
        "    for _ in range(num_layers):\n",
        "        conv_layer = random.choice([Conv2D, SeparableConv2D])\n",
        "        filters = random.randint(16, 128)\n",
        "        kernel_size = random.choice([(3, 3), (5, 5)])\n",
        "        activation = random.choice(['relu', 'leaky_relu', 'elu', 'selu', 'swish'])\n",
        "\n",
        "        if activation == 'selu':\n",
        "            model.add(conv_layer(filters=filters, kernel_size=kernel_size, activation='selu', padding='same'))\n",
        "        else:\n",
        "            model.add(conv_layer(filters=filters, kernel_size=kernel_size, padding='same'))\n",
        "            if activation == 'leaky_relu':\n",
        "                model.add(LeakyReLU(alpha=0.01))\n",
        "            elif activation == 'elu':\n",
        "                model.add(ELU(alpha=1.0))\n",
        "            elif activation == 'swish':\n",
        "                model.add(Activation(tf.nn.swish))\n",
        "            else:  # relu\n",
        "                model.add(Activation(activation))\n",
        "\n",
        "        model.add(BatchNormalization())\n",
        "        if random.random() < 0.5:\n",
        "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        if random.random() < 0.5:\n",
        "            model.add(Dropout(rate=random.uniform(0.2, 0.5)))\n",
        "\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(units=random.randint(64, 128), activation='relu'))\n",
        "    if random.random() < 0.5:\n",
        "        model.add(Dropout(rate=random.uniform(0.2, 0.5)))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Пример использования\n",
        "model = create_random_cnn_model(input_shape=(28, 28, 1), num_classes=10)\n",
        "model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "F7kvaDlasgEP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset_with_overlap(data, labels, num_chunks, overlap=0.2):\n",
        "    \"\"\"\n",
        "    Делит набор данных на `num_chunks` частей с перекрытием `overlap`.\n",
        "\n",
        "    Parameters:\n",
        "    data (np.ndarray): Набор данных для деления.\n",
        "    labels (np.ndarray): Метки классов соответствующих данных.\n",
        "    num_chunks (int): Количество частей, на которые будет разделен набор данных.\n",
        "    overlap (float): Доля перекрытия между последовательными частями.\n",
        "\n",
        "    Returns:\n",
        "    list of tuples: Список кортежей, где каждый кортеж содержит часть данных и соответствующие метки.\n",
        "    \"\"\"\n",
        "    chunk_size = len(data) // num_chunks\n",
        "    overlap_size = int(chunk_size * overlap)\n",
        "\n",
        "    chunks = []\n",
        "    for i in range(num_chunks):\n",
        "        start = i * chunk_size - i * overlap_size\n",
        "        end = start + chunk_size + overlap_size\n",
        "\n",
        "        # Зацикливание данных для обеспечения перекрытия\n",
        "        if end > len(data):\n",
        "            end -= len(data)\n",
        "            chunk_data = np.concatenate((data[start:], data[:end]))\n",
        "            chunk_labels = np.concatenate((labels[start:], labels[:end]))\n",
        "        else:\n",
        "            chunk_data = data[start:end]\n",
        "            chunk_labels = labels[start:end]\n",
        "\n",
        "        chunks.append((chunk_data, chunk_labels))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Пример использования\n",
        "# Предположим, у вас есть переменные x_train и y_train, содержащие тренировочные данные и метки соответственно\n",
        "num_chunks = 10  # Например, разделим на 5 частей\n",
        "chunks = split_dataset_with_overlap(x_train, y_train, num_chunks, overlap=0.2)\n",
        "\n",
        "# Теперь вы можете обучить модель на каждом чанке данных\n",
        "for i, (chunk_data, chunk_labels) in enumerate(chunks):\n",
        "    print(f\"Размер чанка №{i+1} = {chunk_data.shape}\")\n",
        "    # Обучите вашу модель так\n",
        "    # Пример: model.fit(chunk_data, chunk_labels, epochs=10, batch_size=128)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rTh2pPuFUDR",
        "outputId": "c02604fa-360b-4dea-be40-1761306e735f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер чанка№1 = (7200, 28, 28, 1)\n",
            "Размер чанка№2 = (7200, 28, 28, 1)\n",
            "Размер чанка№3 = (7200, 28, 28, 1)\n",
            "Размер чанка№4 = (7200, 28, 28, 1)\n",
            "Размер чанка№5 = (7200, 28, 28, 1)\n",
            "Размер чанка№6 = (7200, 28, 28, 1)\n",
            "Размер чанка№7 = (7200, 28, 28, 1)\n",
            "Размер чанка№8 = (7200, 28, 28, 1)\n",
            "Размер чанка№9 = (7200, 28, 28, 1)\n",
            "Размер чанка№10 = (7200, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Генерируем 10 случайные модели\n",
        "models = [create_random_cnn_model(input_shape=x_train.shape[1:], num_classes=num_classes) for _ in range(10)]\n",
        "\n",
        "# Компилируем модели\n",
        "for model in models:\n",
        "    model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "# Сохранение моделей\n",
        "save_dir = \"/content/drive/MyDrive/all_difficulty_data_mnist/weights_100rcnn_normal_train_data\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Обучение моделей на соответствующих чанках\n",
        "for i, (model, (chunk_data, chunk_labels)) in enumerate(zip(models, chunks)):\n",
        "    print(f\"Обучение модели {i+1} на чанке {i+1}\")\n",
        "    model.fit(chunk_data, chunk_labels, epochs=10, batch_size=128, validation_split=0.2, verbose=1)\n",
        "    # Сохранение модели\n",
        "    model_path = os.path.join(save_dir, f\"model_{i+1}.h5\")\n",
        "    model.save(model_path)\n",
        "    print(f\"Модель {i+1} сохранена в {model_path}\")\n",
        "\n",
        "# Тестирование моделей на тестовом наборе данных\n",
        "for i, model in enumerate(models):\n",
        "    print(f\"Тестирование модели {i+1}\")\n",
        "    loss, accuracy = model.evaluate(x_test, y_test_ohe, verbose=0)\n",
        "    print(f\"Точность модели {i+1} на тестовых данных: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vpbc0K49xE_Y",
        "outputId": "460464d7-2e35-4c8f-f05b-01f6abac61cb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обучение модели 1 на чанке 1\n",
            "Epoch 1/10\n",
            "45/45 [==============================] - 1s 17ms/step - loss: 0.0423 - accuracy: 0.9861 - val_loss: 0.0577 - val_accuracy: 0.9840\n",
            "Epoch 2/10\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.0339 - accuracy: 0.9894 - val_loss: 0.0620 - val_accuracy: 0.9868\n",
            "Epoch 3/10\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.0333 - accuracy: 0.9889 - val_loss: 0.0423 - val_accuracy: 0.9882\n",
            "Epoch 4/10\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.0288 - accuracy: 0.9917 - val_loss: 0.0488 - val_accuracy: 0.9861\n",
            "Epoch 5/10\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.0266 - accuracy: 0.9903 - val_loss: 0.0428 - val_accuracy: 0.9896\n",
            "Epoch 6/10\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.0225 - accuracy: 0.9922 - val_loss: 0.0501 - val_accuracy: 0.9861\n",
            "Epoch 7/10\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.0224 - accuracy: 0.9934 - val_loss: 0.0405 - val_accuracy: 0.9903\n",
            "Epoch 8/10\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.0225 - accuracy: 0.9922 - val_loss: 0.0422 - val_accuracy: 0.9847\n",
            "Epoch 9/10\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.0232 - accuracy: 0.9931 - val_loss: 0.0554 - val_accuracy: 0.9847\n",
            "Epoch 10/10\n",
            "45/45 [==============================] - 0s 10ms/step - loss: 0.0213 - accuracy: 0.9924 - val_loss: 0.0579 - val_accuracy: 0.9833\n",
            "Модель 1 сохранена в /content/drive/MyDrive/all_difficulty_data_mnist/weights_100rcnn_normal_train_data/model_1.h5\n",
            "Обучение модели 2 на чанке 2\n",
            "Epoch 1/10\n",
            " 1/45 [..............................] - ETA: 0s - loss: 0.2503 - accuracy: 0.8984"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45/45 [==============================] - 1s 13ms/step - loss: 0.1990 - accuracy: 0.9408 - val_loss: 0.0836 - val_accuracy: 0.9729\n",
            "Epoch 2/10\n",
            "45/45 [==============================] - 0s 11ms/step - loss: 0.1920 - accuracy: 0.9406 - val_loss: 0.1058 - val_accuracy: 0.9660\n",
            "Epoch 3/10\n",
            "45/45 [==============================] - 0s 11ms/step - loss: 0.1907 - accuracy: 0.9450 - val_loss: 0.1742 - val_accuracy: 0.9472\n",
            "Epoch 4/10\n",
            "45/45 [==============================] - 0s 11ms/step - loss: 0.1770 - accuracy: 0.9486 - val_loss: 0.1307 - val_accuracy: 0.9542\n",
            "Epoch 5/10\n",
            "45/45 [==============================] - 1s 11ms/step - loss: 0.1665 - accuracy: 0.9490 - val_loss: 0.1367 - val_accuracy: 0.9528\n",
            "Epoch 6/10\n",
            "45/45 [==============================] - 0s 11ms/step - loss: 0.1745 - accuracy: 0.9443 - val_loss: 0.1182 - val_accuracy: 0.9611\n",
            "Epoch 7/10\n",
            "45/45 [==============================] - 1s 11ms/step - loss: 0.1770 - accuracy: 0.9479 - val_loss: 0.1079 - val_accuracy: 0.9639\n",
            "Epoch 8/10\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 0.1607 - accuracy: 0.9538 - val_loss: 0.1886 - val_accuracy: 0.9368\n",
            "Epoch 9/10\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 0.1655 - accuracy: 0.9510 - val_loss: 0.1070 - val_accuracy: 0.9694\n",
            "Epoch 10/10\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 0.1673 - accuracy: 0.9505 - val_loss: 0.0883 - val_accuracy: 0.9708\n",
            "Модель 2 сохранена в /content/drive/MyDrive/all_difficulty_data_mnist/weights_100rcnn_normal_train_data/model_2.h5\n",
            "Обучение модели 3 на чанке 3\n",
            "Epoch 1/10\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 0.0500 - accuracy: 0.9852 - val_loss: 0.0788 - val_accuracy: 0.9806\n",
            "Epoch 2/10\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.0434 - accuracy: 0.9887 - val_loss: 0.0726 - val_accuracy: 0.9757\n",
            "Epoch 3/10\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.0417 - accuracy: 0.9875 - val_loss: 0.0947 - val_accuracy: 0.9667\n",
            "Epoch 4/10\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.0393 - accuracy: 0.9882 - val_loss: 0.0952 - val_accuracy: 0.9653\n",
            "Epoch 5/10\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.0352 - accuracy: 0.9885 - val_loss: 0.0986 - val_accuracy: 0.9694\n",
            "Epoch 6/10\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.0276 - accuracy: 0.9937 - val_loss: 0.0610 - val_accuracy: 0.9771\n",
            "Epoch 7/10\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.0305 - accuracy: 0.9917 - val_loss: 0.1498 - val_accuracy: 0.9521\n",
            "Epoch 8/10\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.0285 - accuracy: 0.9931 - val_loss: 0.0812 - val_accuracy: 0.9722\n",
            "Epoch 9/10\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 0.0229 - accuracy: 0.9948 - val_loss: 0.1486 - val_accuracy: 0.9535\n",
            "Epoch 10/10\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 0.0292 - accuracy: 0.9901 - val_loss: 0.0881 - val_accuracy: 0.9715\n",
            "Модель 3 сохранена в /content/drive/MyDrive/all_difficulty_data_mnist/weights_100rcnn_normal_train_data/model_3.h5\n",
            "Тестирование модели 1\n",
            "Точность модели 1 на тестовых данных: 98.35%\n",
            "Тестирование модели 2\n",
            "Точность модели 2 на тестовых данных: 96.67%\n",
            "Тестирование модели 3\n",
            "Точность модели 3 на тестовых данных: 97.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "теперь давай применим к тестовым данным\n",
        "def generate_deformed_data(x_data, deform_params):(еще не задана в гугл коллабе)\n",
        "    # Создание словаря для хранения деформированных данных по уровням сложности\n",
        "    deformed_data = {}\n",
        "    for level, params in deform_params.items():\n",
        "        A, B = params\n",
        "        # Деформация всех тестовых изображений согласно текущим параметрам деформации\n",
        "        deformed_images = np.array([deform_image_optimized(image.reshape(28, 28), A, B) for image in x_data]).reshape(-1, 28, 28, 1)\n",
        "        deformed_data[level] = deformed_images\n",
        "    return deformed_data\n",
        "\n",
        "# Генерация деформированных тестовых данных для каждого уровня сложности\n",
        "deformed_test_data = generate_deformed_data(x_test, deform_params)\n",
        "с параметрами(уже заданы в коде коллаба выше\n",
        "deform_params = {\n",
        "    \"1\": (0.2, 1.0),\n",
        "    \"2\": (0.4, 3.0),\n",
        "    \"3\": (1, 5.0),\n",
        "    \"4\": (3.5, 9.5),\n",
        "    \"5\": (1, 5.0),\n",
        "}\n",
        "нужно поделить тестовый набор первые 1 тыс. экзмплряов из тестовых данных это деформация с параметрами 1, вторая тысяча с параметрами 2 и тд.\n",
        "нужно создать тестовые деофрмированые данные всех видов 1 - 5 объеденить с\n",
        "0 уровнем(возьми первую 1 тыс.) и 6 уровнем(возьми полсдение 1 тыс.)\n",
        "6 уровню дай лейбл спец класс 10 не знаю\n",
        "# Загрузка данных для уровня сложности 0\n",
        "x_data_level_0 = np.load('/content/drive/MyDrive/all_difficulty_data_mnist/x_data_difficult_level_0.npy')\n",
        "y_data_level_0 = np.load('/content/drive/MyDrive/all_difficulty_data_mnist/y_data_difficult_level_0.npy')\n",
        "\n",
        "# Загрузка данных для уровня сложности 6\n",
        "x_data_level_6 = np.load('/content/drive/MyDrive/all_difficulty_data_mnist/x_data_difficult_level_6.npy')\n",
        "y_data_level_6 = np.load('/content/drive/MyDrive/all_difficulty_data_mnist/y_data_difficult_level_6.npy')\n",
        "\n",
        "они уже дефорвмированы\n",
        "потом прогнать каждую сеть и посчитать точность и таблицы cvs где будет номер сети\n",
        "+1 если правильный ответ 0 если ответ не знаю -1 если неправлиьный"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "BO82MdlhJLGs",
        "outputId": "22131bba-4402-4fed-ba4e-d3febaba8637"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-2-8ac04ee40d02>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-8ac04ee40d02>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    теперь давай применим к тестовым данным\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NcUYXCNBJQYM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}