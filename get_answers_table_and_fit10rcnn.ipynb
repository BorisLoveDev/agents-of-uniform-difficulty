{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BorisLoveDev/agents-of-uniform-difficulty/blob/main/get_answers_table_and_fit10rcnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-LESJJ9u7bjU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "import os\n",
        "import random\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNm_F8qC7dug",
        "outputId": "2a10d8db-9fa5-44f9-8f99-3629b045ac6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# Подключение Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6SUnaDC7ogF",
        "outputId": "9d945d45-37d8-4ab4-cc30-9614095b7a62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Загрузка и нормализация данных MNIST\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# Добавление измерения канала к данным\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "# Применение one-hot encoding\n",
        "y_test_ohe = to_categorical(y_test, num_classes)\n",
        "y_train_ohe = to_categorical(y_train, num_classes)\n",
        "y_train_ohe.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9MoPMuvofpYT"
      },
      "outputs": [],
      "source": [
        "# Загрузка данных для уровня сложности 0\n",
        "x_data_level_0 = np.load('/content/drive/MyDrive/all_difficulty_data_mnist/x_data_difficult_level_0.npy')\n",
        "y_data_level_0 = np.load('/content/drive/MyDrive/all_difficulty_data_mnist/y_data_difficult_level_0.npy')\n",
        "\n",
        "# Загрузка данных для уровня сложности 6\n",
        "x_data_level_6 = np.load('/content/drive/MyDrive/all_difficulty_data_mnist/x_data_difficult_level_6.npy')\n",
        "y_data_level_6 = np.load('/content/drive/MyDrive/all_difficulty_data_mnist/y_data_difficult_level_6.npy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IDDXpVxmATsD"
      },
      "outputs": [],
      "source": [
        "# Функция для деформации изображения, как определено ранее\n",
        "def deform_image_optimized(image, A, B, M=28, NP=5):\n",
        "    C = M / (NP + 1.0)\n",
        "    XN, YN = np.zeros(M), np.zeros(M)\n",
        "    DX, DY = np.linspace(0, M-1, M), np.linspace(0, M-1, M)\n",
        "\n",
        "    for j in range(NP):\n",
        "        TXN = (j + 0.5 - np.random.random()) * C\n",
        "        TYN = (j + 0.5 - np.random.random()) * C\n",
        "        TDX = (j + 0.5 - np.random.random()) * C\n",
        "        TDY = (j + 0.5 - np.random.random()) * C\n",
        "        AXN = B * (1.0 - 2.0 * np.random.random())\n",
        "        AYN = B * (1.0 - 2.0 * np.random.random())\n",
        "        ADX = A * (1.0 - 2.0 * np.random.random())\n",
        "        ADY = A * (1.0 - 2.0 * np.random.random())\n",
        "        PXN = (0.1 + 0.9 * np.random.random()) * C\n",
        "        PYN = (0.1 + 0.9 * np.random.random()) * C\n",
        "        PDX = (0.1 + 0.9 * np.random.random()) * C\n",
        "        PDY = (0.1 + 0.9 * np.random.random()) * C\n",
        "\n",
        "        DX += ADX * np.exp(-((DX - TDX) / PDX)**2)\n",
        "        DY += ADY * np.exp(-((DY - TDY) / PDY)**2)\n",
        "        XN += AXN * np.exp(-((DX - TXN) / PXN)**2)\n",
        "        YN += AYN * np.exp(-((DY - TYN) / PYN)**2)\n",
        "\n",
        "    deformed_image = np.zeros((M, M))\n",
        "    for j in range(M):\n",
        "        for i in range(M):\n",
        "            x_index = int(DX[i] + XN[j])\n",
        "            y_index = int(DY[j] + YN[i])\n",
        "            if 0 <= x_index < M and 0 <= y_index < M:\n",
        "                deformed_image[j, i] = image[y_index, x_index] if y_index < image.shape[0] and x_index < image.shape[1] else 0\n",
        "\n",
        "    return deformed_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "faAey5shAUeY"
      },
      "outputs": [],
      "source": [
        "deform_params = {\n",
        "    \"1\": (0.2, 1.0),\n",
        "    \"2\": (0.4, 3.0),\n",
        "    \"3\": (1, 5.0),\n",
        "    \"4\": (3.5, 9.5),\n",
        "    \"5\": (1, 5.0),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbinBMGIMtGo"
      },
      "source": [
        "Получение таблицы ответов для 10 акторов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kjYl2Tvlnno9"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, SeparableConv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, Dense, Dropout, Flatten, BatchNormalization, Activation, LeakyReLU, ELU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "F7kvaDlasgEP"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_random_cnn_model(input_shape=(28, 28, 1), num_classes=10):\n",
        "    model = Sequential()\n",
        "\n",
        "    conv_layer = random.choice([Conv2D, SeparableConv2D])\n",
        "    filters = random.randint(16, 64)\n",
        "    kernel_size = random.choice([(3, 3), (5, 5)])\n",
        "    activation = random.choice(['relu', 'leaky_relu', 'elu', 'selu', 'swish'])\n",
        "\n",
        "    model.add(conv_layer(filters=filters, kernel_size=kernel_size, activation=activation, input_shape=input_shape))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    num_layers = random.randint(1, 3)\n",
        "    for _ in range(num_layers):\n",
        "        conv_layer = random.choice([Conv2D, SeparableConv2D])\n",
        "        filters = random.randint(16, 128)\n",
        "        kernel_size = random.choice([(3, 3), (5, 5)])\n",
        "        activation = random.choice(['relu', 'leaky_relu', 'elu', 'selu', 'swish'])\n",
        "\n",
        "        if activation == 'selu':\n",
        "            model.add(conv_layer(filters=filters, kernel_size=kernel_size, activation='selu', padding='same'))\n",
        "        else:\n",
        "            model.add(conv_layer(filters=filters, kernel_size=kernel_size, padding='same'))\n",
        "            if activation == 'leaky_relu':\n",
        "                model.add(LeakyReLU(alpha=0.01))\n",
        "            elif activation == 'elu':\n",
        "                model.add(ELU(alpha=1.0))\n",
        "            elif activation == 'swish':\n",
        "                model.add(Activation(tf.nn.swish))\n",
        "            else:  # relu\n",
        "                model.add(Activation(activation))\n",
        "\n",
        "        model.add(BatchNormalization())\n",
        "        if random.random() < 0.5:\n",
        "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        if random.random() < 0.5:\n",
        "            model.add(Dropout(rate=random.uniform(0.2, 0.5)))\n",
        "\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(units=random.randint(64, 128), activation='relu'))\n",
        "    if random.random() < 0.5:\n",
        "        model.add(Dropout(rate=random.uniform(0.2, 0.5)))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Пример использования\n",
        "model = create_random_cnn_model(input_shape=(28, 28, 1), num_classes=10)\n",
        "model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-rTh2pPuFUDR"
      },
      "outputs": [],
      "source": [
        "def split_dataset_with_overlap(data, labels, num_chunks, overlap=0.2):\n",
        "    chunk_size = len(data) // num_chunks\n",
        "    overlap_size = int(chunk_size * overlap)\n",
        "\n",
        "    chunks = []\n",
        "    for i in range(num_chunks):\n",
        "        start = i * chunk_size - i * overlap_size\n",
        "        end = start + chunk_size + overlap_size\n",
        "\n",
        "        if end > len(data):\n",
        "            end -= len(data)\n",
        "            chunk_data = np.concatenate((data[start:], data[:end]))\n",
        "            chunk_labels = np.concatenate((labels[start:], labels[:end]))\n",
        "        else:\n",
        "            chunk_data = data[start:end]\n",
        "            chunk_labels = labels[start:end]\n",
        "\n",
        "        chunks.append((chunk_data, chunk_labels))\n",
        "\n",
        "    data_shapes = [chunk[0].shape for chunk in chunks]\n",
        "    labels_shapes = [chunk[1].shape for chunk in chunks]\n",
        "\n",
        "    print(f\"Формы данных: {data_shapes}\")\n",
        "    print(f\"Формы меток: {labels_shapes}\")\n",
        "\n",
        "    return chunks  # Возвращаем список чанков\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yjeF6KsQkxfl"
      },
      "outputs": [],
      "source": [
        "def defromed_chunk(chunk_data, chunk_labels, deform_params1, deform_params2):\n",
        "    # Извлекаем параметры деформации\n",
        "    A1, B1 = deform_params1\n",
        "    A2, B2 = deform_params2\n",
        "\n",
        "    augmented_images = []\n",
        "    augmented_labels = []\n",
        "\n",
        "    # Применяем деформацию к первым 500 изображениям с параметрами deform_params1\n",
        "    for image in chunk_data[:500]:\n",
        "        deformed_image = deform_image_optimized(image.squeeze(), A=A1, B=B1)\n",
        "        augmented_images.append(deformed_image.reshape(28, 28, 1))\n",
        "        augmented_labels.append(10)  # Добавляем метку специального класса\n",
        "\n",
        "    # Применяем деформацию к следующим 500 изображениям с параметрами deform_params2\n",
        "    for image in chunk_data[500:1000]:\n",
        "        deformed_image = deform_image_optimized(image.squeeze(), A=A2, B=B2)\n",
        "        augmented_images.append(deformed_image.reshape(28, 28, 1))\n",
        "        augmented_labels.append(10)  # Добавляем метку специального класса\n",
        "\n",
        "    # Преобразуем списки в numpy массивы\n",
        "    augmented_images = np.array(augmented_images)\n",
        "    augmented_labels = np.array(augmented_labels)\n",
        "\n",
        "    # Объединяем исходные данные и метки с дополненными\n",
        "    chunk_data_augmented = np.concatenate((chunk_data, augmented_images))\n",
        "    chunk_labels_augmented = np.concatenate((chunk_labels, augmented_labels))\n",
        "\n",
        "    # Перемешивание дополненных данных и меток\n",
        "    chunk_data_augmented, chunk_labels_augmented = shuffle(chunk_data_augmented, chunk_labels_augmented, random_state=42)\n",
        "    # Вывод информации о форме и распределении\n",
        "    print(f\"Чанк данных форма: {chunk_data_augmented.shape}, Чанк меток форма: {chunk_labels_augmented.shape}\", end='; ')\n",
        "    unique, counts = np.unique(chunk_labels_augmented, return_counts=True)\n",
        "    class_distribution = ', '.join([f\"Класс {u}: {c}\" for u, c in zip(unique, counts)])\n",
        "    print(f\"Распределение по классам: {class_distribution}\")\n",
        "\n",
        "    return chunk_data_augmented, chunk_labels_augmented\n",
        "\n",
        "# Для использования этой функции, вызовите её для каждого чанка данных и меток.\n",
        "# Пример:\n",
        "# first_chunk_data_augmented, first_chunk_labels_augmented = augment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XTJ239rmE3x",
        "outputId": "1088bae8-84fa-4864-f30e-fbaa6bea31c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Формы данных: [(7200, 28, 28, 1), (7200, 28, 28, 1), (7200, 28, 28, 1), (7200, 28, 28, 1), (7200, 28, 28, 1), (7200, 28, 28, 1), (7200, 28, 28, 1), (7200, 28, 28, 1), (7200, 28, 28, 1), (7200, 28, 28, 1)]\n",
            "Формы меток: [(7200,), (7200,), (7200,), (7200,), (7200,), (7200,), (7200,), (7200,), (7200,), (7200,)]\n",
            "Чанк данных форма: (8200, 28, 28, 1), Чанк меток форма: (8200,); Распределение по классам: Класс 0: 713, Класс 1: 809, Класс 2: 694, Класс 3: 735, Класс 4: 734, Класс 5: 629, Класс 6: 725, Класс 7: 770, Класс 8: 673, Класс 9: 718, Класс 10: 1000\n",
            "Чанк данных форма: (8200, 28, 28, 1), Чанк меток форма: (8200,); Распределение по классам: Класс 0: 745, Класс 1: 817, Класс 2: 704, Класс 3: 754, Класс 4: 666, Класс 5: 629, Класс 6: 728, Класс 7: 755, Класс 8: 682, Класс 9: 720, Класс 10: 1000\n",
            "Чанк данных форма: (8200, 28, 28, 1), Чанк меток форма: (8200,); Распределение по классам: Класс 0: 724, Класс 1: 816, Класс 2: 693, Класс 3: 759, Класс 4: 664, Класс 5: 661, Класс 6: 689, Класс 7: 743, Класс 8: 708, Класс 9: 743, Класс 10: 1000\n",
            "Чанк данных форма: (8200, 28, 28, 1), Чанк меток форма: (8200,); Распределение по классам: Класс 0: 704, Класс 1: 844, Класс 2: 681, Класс 3: 759, Класс 4: 693, Класс 5: 651, Класс 6: 698, Класс 7: 720, Класс 8: 704, Класс 9: 746, Класс 10: 1000\n",
            "Чанк данных форма: (8200, 28, 28, 1), Чанк меток форма: (8200,); Распределение по классам: Класс 0: 698, Класс 1: 820, Класс 2: 726, Класс 3: 737, Класс 4: 720, Класс 5: 674, Класс 6: 714, Класс 7: 728, Класс 8: 680, Класс 9: 703, Класс 10: 1000\n",
            "Чанк данных форма: (8200, 28, 28, 1), Чанк меток форма: (8200,); Распределение по классам: Класс 0: 703, Класс 1: 829, Класс 2: 721, Класс 3: 703, Класс 4: 693, Класс 5: 658, Класс 6: 725, Класс 7: 747, Класс 8: 701, Класс 9: 720, Класс 10: 1000\n",
            "Чанк данных форма: (8200, 28, 28, 1), Чанк меток форма: (8200,); Распределение по классам: Класс 0: 703, Класс 1: 824, Класс 2: 707, Класс 3: 723, Класс 4: 703, Класс 5: 634, Класс 6: 711, Класс 7: 735, Класс 8: 714, Класс 9: 746, Класс 10: 1000\n",
            "Чанк данных форма: (8200, 28, 28, 1), Чанк меток форма: (8200,); Распределение по классам: Класс 0: 714, Класс 1: 803, Класс 2: 722, Класс 3: 730, Класс 4: 719, Класс 5: 640, Класс 6: 718, Класс 7: 733, Класс 8: 706, Класс 9: 715, Класс 10: 1000\n",
            "Чанк данных форма: (8200, 28, 28, 1), Чанк меток форма: (8200,); Распределение по классам: Класс 0: 730, Класс 1: 827, Класс 2: 721, Класс 3: 691, Класс 4: 679, Класс 5: 662, Класс 6: 709, Класс 7: 761, Класс 8: 699, Класс 9: 721, Класс 10: 1000\n",
            "Чанк данных форма: (8200, 28, 28, 1), Чанк меток форма: (8200,); Распределение по классам: Класс 0: 713, Класс 1: 784, Класс 2: 722, Класс 3: 755, Класс 4: 690, Класс 5: 652, Класс 6: 706, Класс 7: 757, Класс 8: 729, Класс 9: 692, Класс 10: 1000\n"
          ]
        }
      ],
      "source": [
        "num_chunks = 10  # Например, разделим на 10 частей\n",
        "chunks = split_dataset_with_overlap(x_train, y_train, num_chunks, overlap=0.2)\n",
        "\n",
        "# Определение параметров деформации для чанков\n",
        "deform_params_first_half = (12.0, 32.0)  # Параметры для первых пяти чанков\n",
        "deform_params_second_half = (1.0, 5.0)   # Параметры для последних пяти чанков\n",
        "\n",
        "# Обработка каждого чанка\n",
        "for i, (chunk_data, chunk_labels) in enumerate(chunks):\n",
        "    if i < 5:  # Первые пять чанков\n",
        "        chunk_data_augmented, chunk_labels_augmented = defromed_chunk(\n",
        "            chunk_data, chunk_labels,\n",
        "            deform_params1=deform_params_first_half,\n",
        "            deform_params2=deform_params_first_half)\n",
        "    else:  # Оставшиеся пять чанков\n",
        "        chunk_data_augmented, chunk_labels_augmented = defromed_chunk(\n",
        "            chunk_data, chunk_labels,\n",
        "            deform_params1=deform_params_first_half,\n",
        "            deform_params2=deform_params_second_half)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1-dhK6JGiECh"
      },
      "outputs": [],
      "source": [
        "# 1) Создание генератора-аугментатора данных для классификации MNIST\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R72dfU1AW61V",
        "outputId": "d49ab6ba-c6cb-46f2-b1bb-9252d5fc5e2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Добавление нулевого класса в тестовые метки\n",
        "y_test_extended = np.append(y_test_ohe, 0 * np.ones((len(y_test), 1)), axis=1)\n",
        "y_test_extended[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vpbc0K49xE_Y"
      },
      "outputs": [],
      "source": [
        "num_chunks = len(chunks)  # Общее количество чанков\n",
        "num_classes = 11\n",
        "\n",
        "# Генерация 10 случайных моделей\n",
        "models = [create_random_cnn_model(input_shape=x_train.shape[1:], num_classes=num_classes) for _ in range(10)]\n",
        "\n",
        "# Компиляция моделей\n",
        "for model in models:\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/all_difficulty_data_mnist/weights_100rcnn_normal_train_data\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "for i, (model, (chunk_data, chunk_labels)) in enumerate(zip(models, chunks)):\n",
        "      # Проверка, была ли модель уже обучена (пропускаем первые 5 модели)\n",
        "    if i < 5:\n",
        "        print(f\"Пропуск модели {i+1}, так как она уже обучена.\")\n",
        "        continue  # Переход к следующей модели\n",
        "\n",
        "    # Подготовка валидационного чанка\n",
        "    val_chunk_index = (i + 1) % num_chunks\n",
        "    val_chunk_data, val_chunk_labels = chunks[val_chunk_index]\n",
        "    val_chunk_labels_one_hot = to_categorical(val_chunk_labels, num_classes=num_classes)\n",
        "\n",
        "    chunk_labels_one_hot = to_categorical(chunk_labels, num_classes=num_classes)\n",
        "    print(f\"Обучение модели {i+1}\")\n",
        "    print(model.summary())\n",
        "    # Создание каллбеков\n",
        "    model_checkpoint = ModelCheckpoint(os.path.join(save_dir, f\"best_model_{i+1}.h5\"), save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.001)\n",
        "\n",
        "    if i % 2 == 0:  # Для четных индексов используем аугментированные данные\n",
        "        train_generator = datagen.flow(chunk_data, chunk_labels_one_hot, batch_size=32)\n",
        "        model.fit(train_generator, epochs=100, verbose=1, callbacks=[model_checkpoint, reduce_lr], validation_data=(val_chunk_data, val_chunk_labels_one_hot))\n",
        "    else:  # Для нечетных индексов используем неаугментированные данные\n",
        "        model.fit(chunk_data, chunk_labels_one_hot, epochs=100, batch_size=32, verbose=1, callbacks=[model_checkpoint, reduce_lr], validation_data=(val_chunk_data, val_chunk_labels_one_hot))\n",
        "\n",
        "    # Оценка точности модели на тестовых данных\n",
        "    print(f\"Тестирование модели {i+1}\")\n",
        "    loss, accuracy = model.evaluate(x_test, y_test_extended, verbose=0)\n",
        "    print(f\"Точность модели {i+1} на тестовых данных: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Сохранение модели после обучения\n",
        "    model_path = os.path.join(save_dir, f\"model_{i+1}.h5\")\n",
        "    model.save(model_path)\n",
        "    print(f\"Модель {i+1} сохранена в {model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NcUYXCNBJQYM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}